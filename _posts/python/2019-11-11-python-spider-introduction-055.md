---
layout: post
title:  第55天：爬虫的介绍
category: python
copyright: python
---

by 闲欢

作为程序员，相信大家对“爬虫”这个词并不陌生，身边常常会有人提这个词，在不了解它的人眼中，会觉得这个技术很高端很神秘。不用着急，我们的爬虫系列就是带你去揭开它的神秘面纱，探寻它真实的面目。
<!--more-->

## 爬虫是什么

网络爬虫（又被称为网页蜘蛛，网络机器人），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。

通俗地讲，我们把互联网比作一张大蜘蛛网，每个站点资源比作蜘蛛网上的一个结点，爬虫就像一只蜘蛛，按照设计好的路线和规则在这张蜘蛛网上找到目标结点，获取资源。

## 为什么使用爬虫

为什么我们需要使用爬虫呢？

大家可以想象一下一个场景：你非常崇拜一个微博名人，对他的微博非常着迷，你想把他十年来微博上的每一句话摘抄下来，制作成名人语录。这个时候你怎么办呢？手动去 Ctrl+C 和 Ctrl+V 吗？这种方法确实没错，数据量小的时候我们还可以这样做，但是数据成千上万的时候你还要这样做吗？

我们再来想象另一个场景：你要做一个新闻聚合网站，每天需要定时去几个新闻网站获取最新的新闻，我们称之为 RSS 订阅。难道你会定时去各个订阅网站复制新闻吗？恐怕个人很难做到这一点吧。

上面两种场景，使用爬虫技术可以很轻易地解决问题。所以，我们可以看到，爬虫技术主要可以帮助我们做两类事情：一类是数据获取需求，主要针对特定规则下的大数据量的信息获取；另一类是自动化需求，主要应用在类似信息聚合、搜索等方面。

## 爬虫的分类

从爬取对象来看，爬虫可以分为通用爬虫和聚焦爬虫两类。

通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子 URL 扩充到整个 Web，主要为搜索引擎和大型 Web 服务提供商采集数据。这类网络爬虫的爬取范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低。例如我们常见的百度和谷歌搜索。我们输入关键词，它们会从全网去找关键词相关的网页，并且按照一定的顺序呈现给我们。

聚焦网络爬虫（Focused Crawler），是指选择性地爬取那些与预先定义好的主题相关页面的网络爬虫。和通用网络爬虫相比，聚焦爬虫只需要爬取特定的网页，爬取的广度会小很多。例如我们需要爬取东方财富网的基金数据，我们只需要针对东方财富网的页面制定规则爬取就行。

通俗地讲，通用爬虫就类似于一只蜘蛛，需要寻找特定的食物，但是它不知道蜘蛛网的哪个节点有，所以它只能从一个节点开始寻找，遇到节点就看一下，如果有食物就获取食物，如果这个节点指示某某节点有食物，那它就顺着指示去寻找下个节点。而聚焦网络爬虫就是这只蜘蛛知道哪个节点有食物，它只需要规划好路线到达那个节点就能获取到食物。

## 浏览网页的过程

在用户浏览网页的过程中，我们可能会看到许多好看的图片，比如 http://image.baidu.com/ ，我们会看到几张图片以及百度搜索框，类似下面图片这样：

![baidu_pic_index](http://www.justdopython.com/assets/images/2019/python/baidu_pic_index.png)

这个过程其实就是用户输入网址之后，经过DNS服务器，找到服务器主机，向服务器发出一个请求，服务器经过解析之后，发送给用户的浏览器 HTML、JS、CSS 等文件，浏览器解析出来，用户便可以看到形形色色的图片了。

因此，用户看到的网页实质是由 HTML 代码构成的，爬虫爬来的便是这些内容，通过分析和过滤这些 HTML 代码，实现对图片、文字等资源的获取。

## URL的含义

URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。

> URL 的格式由三部分组成：
>- 第一部分是协议(或称为服务方式)。
>- 第二部分是存有该资源的主机IP地址(有时也包括端口号)。
>- 第三部分是主机资源的具体地址，如目录和文件名等。

由于爬虫的目标是获取资源，而资源都存储在某个主机上，所以爬虫爬取数据时必须要有一个目标的 URL 才可以获取数据，因此，它是爬虫获取数据的基本依据，准确理解它的含义对爬虫学习有很大帮助。

## 爬虫的流程

我们接下来的篇章主要讨论聚焦爬虫，聚焦爬虫的工作流程如下图：

![spider_flow](http://www.justdopython.com/assets/images/2019/python/spider_flow.png)


>- 首先我们需要有一个种子 URL 队列，这个队列中的 URL 相当于我们蜘蛛爬行的第一个结点，是我们在大网中爬行的第一步。
>- 对队列中的每一个 URL 进行请求，我们会得到响应内容，通常响应内容为HTML。如果响应内容里面有我们的目标 URL，提取出来加入 URL 队列中。
>- 解析响应内容，提取我们需要的数据。
>- 存储数据，我们可以将数据存储到数据库、文件等。


从这个爬虫的流程来看，大家应该能够联想到学习爬虫需要学习的关键步骤。首先我们需要像浏览器一样请求某个 URL ，来获取某个主机的资源，那么请求的方法和正确地获取内容就是我们学习的重点。我们获取到资源（也就是请求 URL 之后获得的响应内容）之后，我们需要对响应的内容进行解析，从而获取到对我们有价值的数据，这里面的解析方法就是学习的重点了。我们获取到数据之后，接下来就需要存储数据了，数据的存储方法也很重要。

所以我们学习的爬虫技术，实际上可以归纳为**请求**、**解析**和**存储**三个基本问题。熟练掌握这三个问题对应的解决方法，爬虫技术就算是掌握了。大家在学习爬虫的过程中，紧紧围绕这三个问题展开，就不会走弯路了。

## 总结

本节给大家介绍了爬虫的基本概念，让大家对爬虫有一个大致的了解，以便后续章节的学习。开胃菜吃完了，下一节我们就要开始吃大餐了哟，你准备好了吗？

> 文中示例代码：https://github.com/JustDoPython/python-100-day/tree/master/day-055
